{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.13.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,BatchNormalization,Dropout\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 19:17:52.899275: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-10-07 19:17:52.899344: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-10-07 19:17:52.899362: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-10-07 19:17:52.899495: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-07 19:17:52.899729: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset on https://www.kaggle.com/datasets/raman77768/movie-classifier\n",
    "\n",
    "# Load the dataset\n",
    "movies = pd.read_csv('movie_dataset/train.csv') # Dataset containing genres with movie poster IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7254/7254 [00:10<00:00, 696.92it/s]\n"
     ]
    }
   ],
   "source": [
    "X = [] # Image data\n",
    "# movies.shape[0] = number of iteration required to convert all pictures to numpy array\n",
    "\n",
    "# Normalizing the images to pixel values\n",
    "for i in tqdm(range(movies.shape[0])):\n",
    "    path = 'movie_dataset/Images/' + movies['Id'][i] + '.jpg'\n",
    "\n",
    "    # Load the image and resize it to 350x350 pixels\n",
    "    image = load_img(path, target_size=(350,350,3))\n",
    "\n",
    "    # Convert the image to array\n",
    "    image_array = img_to_array(image)\n",
    "\n",
    "    # Normalize the image\n",
    "    image_array = image_array/255\n",
    "\n",
    "    # Append the image to the list\n",
    "    X.append(np.array(image_array))\n",
    "\n",
    "# Convert the list to numpy array\n",
    "X = np.array(X).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = movies.drop(['Id','Genre'],axis=1)\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=X_train[0].shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(25,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 348, 348, 16)      448       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 348, 348, 16)      64        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 174, 174, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 174, 174, 16)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 172, 172, 32)      4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 172, 172, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 86, 86, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 86, 86, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 84, 84, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 84, 84, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 42, 42, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 42, 42, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 112896)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               14450816  \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                3225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14478585 (55.23 MB)\n",
      "Trainable params: 14478105 (55.23 MB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
       "       'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror',\n",
       "       'Music', 'Musical', 'Mystery', 'N/A', 'News', 'Reality-TV', 'Romance',\n",
       "       'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is a 2D array of shape (n_samples, n_labels)\n",
    "n_labels = y_train.shape[1]\n",
    "class_weights = {}\n",
    "\n",
    "for i in range(n_labels):\n",
    "    # Extract the column for label i and compute class weights\n",
    "    y_train_label_i = y_train[:, i]\n",
    "    weights = compute_class_weight('balanced', classes=[0, 1], y=y_train_label_i)\n",
    "    \n",
    "    # Store the weights in a dictionary\n",
    "    class_weights[i] = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "# Convert class weights to sample weights\n",
    "sample_weights = np.ones(y_train.shape)\n",
    "\n",
    "for i in range(n_labels):\n",
    "    sample_weights[:, i] = np.where(y_train[:, i] == 1, class_weights[i][1], class_weights[i][0])\n",
    "\n",
    "# Lower the Drama class weight by 50%\n",
    "sample_weights[:, 7] *= 0.5\n",
    "# Lower the Romance class weight by 75%\n",
    "sample_weights[:, 18] *= 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduced_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model with ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "204/204 [==============================] - 186s 905ms/step - loss: 0.2243 - accuracy: 0.3480 - val_loss: 0.2922 - val_accuracy: 0.2080 - lr: 1.0000e-04\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 190s 933ms/step - loss: 0.2160 - accuracy: 0.3716 - val_loss: 0.2950 - val_accuracy: 0.2039 - lr: 1.0000e-04\n",
      "Epoch 3/3\n",
      "204/204 [==============================] - 197s 963ms/step - loss: 0.2101 - accuracy: 0.3727 - val_loss: 0.3115 - val_accuracy: 0.2328 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# use this for general model\n",
    "history = model.fit(X_train,y_train,epochs=3,validation_data=(X_test,y_test), batch_size=32, callbacks=[checkpoint, reduced_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
